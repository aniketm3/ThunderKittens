#include <iostream>
#include <fstream>
#include <chrono>
#include <cmath>
#include <string>
#include <cassert>
#include <cuda_runtime.h>
#include "delta_attn_def.h" 

// Include your kernel header(s)
//#include "delta_attn.cu"

// Constants (modify as needed)
constexpr int ATTN_B     = 16;      // Batch size (likely fine as is)
constexpr int QO_HEADS   = 8;       // Update from 16 to 8
constexpr int KV_HEADS   = 8;       // Update from 16 to 8
constexpr int ATTN_N     = 128;     // Update from 768 to 128
constexpr int ATTN_D     = 16;      // Update from 128 to 64
constexpr int ITER       = 10;      // This is probably fine
constexpr bool causal    = true;    // This is probably fine

// Helper function to check CUDA errors.
#define CudaCheckError()    __cudaCheckError( __FILE__, __LINE__ )
inline void __cudaCheckError( const char *file, const int line ) {
    cudaError_t err = cudaGetLastError();
    if ( cudaSuccess != err ) {
        fprintf( stderr, "cudaCheckError() failed at %s:%i : %s\n",
                 file, line, cudaGetErrorString( err ) );
        exit( -1 );
    }
    err = cudaDeviceSynchronize();
    if( cudaSuccess != err ) {
        fprintf( stderr, "cudaCheckError() with sync failed at %s:%i : %s\n",
                 file, line, cudaGetErrorString( err ) );
        exit( -1 );
    }
}

//---------------------------------------------------------------------
// Forward test function
//---------------------------------------------------------------------
void runForwardTest(const char* filename) {
    std::cout << "Running Forward Test..." << std::endl;
    // Calculate total elements for QO (assuming q, k, v, o have the same size)
    constexpr int TOTAL_ELEMENTS_QO = ATTN_B * QO_HEADS * ATTN_N * ATTN_D;

    // Allocate host arrays for inputs and reference outputs.
    float* q      = new float[TOTAL_ELEMENTS_QO];
    float* k      = new float[TOTAL_ELEMENTS_QO];
    float* v      = new float[TOTAL_ELEMENTS_QO];
    float* o_ref  = new float[TOTAL_ELEMENTS_QO]; // reference forward output

    bf16 *q_bf = new bf16[TOTAL_ELEMENTS];
    bf16 *k_bf = new bf16[TOTAL_ELEMENTS];
    bf16 *v_bf = new bf16[TOTAL_ELEMENTS];
    bf16 *o_bf = new bf16[TOTAL_ELEMENTS];
    float *o = new float[TOTAL_ELEMENTS];


    // Open file and read data (assume file order: q, k, v, then o_ref)
    std::ifstream infile(filename);
    if (!infile.is_open()) {
        std::cerr << "Could not open test file: " << filename << std::endl;
        exit(-1);
    }
    for (int i = 0; i < TOTAL_ELEMENTS_QO/ATTN_B; i++) infile >> q[i];
    for (int i = 0; i < TOTAL_ELEMENTS_QO/ATTN_B; i++) infile >> k[i];
    for (int i = 0; i < TOTAL_ELEMENTS_QO/ATTN_B; i++) infile >> v[i];
    for (int i = 0; i < TOTAL_ELEMENTS_QO/ATTN_B; i++) infile >> o_ref[i];
    std::cout << "Finished loading" << std::endl;

    // float dummy;
    // int leftoverSize = TOTAL_ELEMENTS_QO; // figure out how big each leftover array is
  
    // for (int i = 0; i < leftoverSize; i++) infile >> dummy; // d_vec
    // for (int i = 0; i < leftoverSize; i++) infile >> dummy; // og
    // for (int i = 0; i < leftoverSize; i++) infile >> dummy; // qg
    // for (int i = 0; i < leftoverSize; i++) infile >> dummy; // kg
    // for (int i = 0; i < leftoverSize; i++) infile >> dummy; // vg

    // replicate into batch elements
    for(int i = 0; i < TOTAL_ELEMENTS; i++) {
        q_bf[i] = __float2bfloat16(q[i % (TOTAL_ELEMENTS_QO/ATTN_B)]);
        k_bf[i] = __float2bfloat16(k[i % (TOTAL_ELEMENTS_QO/ATTN_B)]);
        v_bf[i] = __float2bfloat16(v[i % (TOTAL_ELEMENTS_QO/ATTN_B)]);
    }

    //infile.close();

    // Allocate device memory for forward inputs and outputs.
    bf16 *d_q, *d_k, *d_v, *d_o;
    cudaMalloc(&d_q, TOTAL_ELEMENTS_QO * sizeof(bf16));
    cudaMalloc(&d_k, TOTAL_ELEMENTS_QO * sizeof(bf16));
    cudaMalloc(&d_v, TOTAL_ELEMENTS_QO * sizeof(bf16));
    cudaMalloc(&d_o, TOTAL_ELEMENTS_QO * sizeof(bf16));

    // Copy inputs to device.
    cudaMemcpy(d_q, q_bf, TOTAL_ELEMENTS_QO * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_k, k_bf, TOTAL_ELEMENTS_QO * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_v, v_bf, TOTAL_ELEMENTS_QO * sizeof(bf16), cudaMemcpyHostToDevice);

    // Initialize global objects for forward kernel.
    // (Assuming you have types such as q_global, k_global, etc. defined in your kernel code.)
    // Here, we assume a type 'globals' that encapsulates forward pointers and dimensions.
    // q_global q_arg{d_q, ATTN_B, QO_HEADS, ATTN_N, ATTN_D};
    // k_global k_arg{d_k, ATTN_B, KV_HEADS, ATTN_N, ATTN_D};
    // v_global v_arg{d_v, ATTN_B, KV_HEADS, ATTN_N, ATTN_D};
    // o_global o_arg{d_o, ATTN_B, QO_HEADS, ATTN_N, ATTN_D};
    // l_global l_arg{/* not used in pure forward test, can be dummy */ nullptr, 0,0,0,0};
    // globals g{q_arg, k_arg, v_arg, o_arg, l_arg, ATTN_N, (QO_HEADS / KV_HEADS)};

    global_layout<ATTN_D> q(d_q, ATTN_B, ATTN_N, QO_HEADS, nullptr);
    global_layout<ATTN_D> k(d_k, ATTN_B, ATTN_N, QO_HEADS, nullptr);
    global_layout<ATTN_D> v(d_v, ATTN_B, ATTN_N, QO_HEADS, nullptr);
    global_layout<ATTN_D> o(d_o, ATTN_B, ATTN_N, QO_HEADS, nullptr);
    globals<ATTN_D> g(q, k, v, o);

    // Set kernel shared memory size.
    unsigned long mem_size = kittens::MAX_SHARED_MEMORY / 2;
    cudaFuncSetAttribute(fwd_attend_ker<ATTN_D, causal>, cudaFuncAttributeMaxDynamicSharedMemorySize, mem_size);

    // Define grid and block dimensions.
    dim3 grid(ATTN_N/(CONSUMER_WARPGROUPS * kittens::TILE_ROW_DIM<bf16>*4), QO_HEADS, ATTN_B);
    dim3 block(32 * NUM_WORKERS);

    // Warmup and benchmark forward kernel.
    for (int i = 0; i < ITER; i++) {
        fwd_attend_ker<ATTN_D, causal><<<grid, block, mem_size>>>(g);
    }
    cudaDeviceSynchronize();

    auto start = std::chrono::high_resolution_clock::now();
    for (int i = 0; i < ITER; i++) {
        fwd_attend_ker<ATTN_D, causal><<<grid, block, mem_size>>>(g);
    }
    cudaDeviceSynchronize();
    auto finish = std::chrono::high_resolution_clock::now();

    // Copy forward output from device.
    float* o = new float[TOTAL_ELEMENTS_QO];
    cudaMemcpy(o, d_o, TOTAL_ELEMENTS_QO * sizeof(float), cudaMemcpyDeviceToHost);

    // Verify forward output against reference.
    double total_diff = 0.0;
    double max_diff = 0.0;
    for (int i = 0; i < TOTAL_ELEMENTS_QO; i++) {
        double diff = std::fabs(o[i] - o_ref[i]);
        total_diff += diff;
        if (diff > max_diff) max_diff = diff;
    }
    std::cout << "Forward Test Results:" << std::endl;
    std::cout << "  Average difference: " << total_diff / TOTAL_ELEMENTS_QO << std::endl;
    std::cout << "  Max difference: " << max_diff << std::endl;

    // Report kernel execution time.
    auto time_us = std::chrono::duration_cast<std::chrono::microseconds>(finish - start).count() / ITER;
    std::cout << "  Average kernel execution time: " << time_us << " us" << std::endl;

    // Cleanup forward test allocations.
    delete[] q; delete[] k; delete[] v; delete[] o_ref; delete[] o;
    cudaFree(d_q); cudaFree(d_k); cudaFree(d_v); cudaFree(d_o);
}

//---------------------------------------------------------------------
// Backward test function
//---------------------------------------------------------------------
//void runBackwardTest(const char* filename) {
    std::cout << "Running Backward Test..." << std::endl;
    // For the backward test, we assume the file contains reference gradients.
    // Let TOTAL_ELEMENTS_QO and TOTAL_ELEMENTS_KV be as defined below.
    constexpr int TOTAL_ELEMENTS_QO = ATTN_B * QO_HEADS * ATTN_N * ATTN_D;
    constexpr int TOTAL_ELEMENTS_KV = ATTN_B * KV_HEADS * ATTN_N * ATTN_D;

    // Allocate host arrays for reference gradients.
    float* qg_ref = new float[TOTAL_ELEMENTS_QO];
    float* kg_ref = new float[TOTAL_ELEMENTS_KV];
    float* vg_ref = new float[TOTAL_ELEMENTS_KV];

    // Open file and read reference gradient data.
    // Assume file order: qg_ref, then kg_ref, then vg_ref.
    std::ifstream infile(filename);
    if (!infile.is_open()) {
        std::cerr << "Could not open gradient test file: " << filename << std::endl;
        exit(-1);
    }
    for (int i = 0; i < 4 * TOTAL_ELEMENTS_QO; i++) { float dummy; infile >> dummy; }

    // skip d_vec
    for (int i = 0; i < TOTAL_ELEMENTS_QO; i++) { float dummy; infile >> dummy; }

    // skip og
    for (int i = 0; i < TOTAL_ELEMENTS_QO; i++) { float dummy; infile >> dummy; }

    // now read QG, KG, VG
    for (int i = 0; i < TOTAL_ELEMENTS_QO; i++) infile >> qg_ref[i];
    for (int i = 0; i < TOTAL_ELEMENTS_QO; i++) infile >> kg_ref[i];
    for (int i = 0; i < TOTAL_ELEMENTS_QO; i++) infile >> vg_ref[i];
    infile.close();

    // Allocate device memory for gradients.
    float *d_qg, *d_kg, *d_vg;
    cudaMalloc(&d_qg, TOTAL_ELEMENTS_QO * sizeof(float));
    cudaMalloc(&d_kg, TOTAL_ELEMENTS_KV * sizeof(float));
    cudaMalloc(&d_vg, TOTAL_ELEMENTS_KV * sizeof(float));

    // Set up backward global structures.
    // (Assuming types bwd_q_global, bwd_k_global, etc. exist in your code.)
    bwd_q_global  bwd_q_arg { /* d_q pointer from forward pass (assumed pre-set) */ nullptr, ATTN_B, QO_HEADS, ATTN_N, ATTN_D};
    bwd_k_global  bwd_k_arg { /* d_k pointer from forward pass */ nullptr, ATTN_B, KV_HEADS, ATTN_N, ATTN_D};
    bwd_v_global  bwd_v_arg { /* d_v pointer from forward pass */ nullptr, ATTN_B, KV_HEADS, ATTN_N, ATTN_D};

    // Also pass pointers for output gradients (d_qg, d_kg, d_vg) and any other required arguments.
    bwd_og_global bwd_og_arg{/* device pointer for og */ nullptr, ATTN_B, QO_HEADS, ATTN_N, ATTN_D};
    bwd_qg_global bwd_qg_arg{ d_qg, ATTN_B, QO_HEADS, ATTN_N, ATTN_D };
    bwd_kg_global bwd_kg_arg{ d_kg, ATTN_B, KV_HEADS, ATTN_N, ATTN_D };
    bwd_vg_global bwd_vg_arg{ d_vg, ATTN_B, KV_HEADS, ATTN_N, ATTN_D };
    bwd_l_global  bwd_l_arg { /* may reuse d_l from forward */ nullptr, ATTN_B, QO_HEADS, 1, ATTN_N };
    bwd_d_global  bwd_d_arg { /* may reuse d_d from forward */ nullptr, ATTN_B, QO_HEADS, 1, ATTN_N };

    bwd_global_args bwd_global{bwd_q_arg, bwd_k_arg, bwd_v_arg, bwd_og_arg,
                                bwd_qg_arg, bwd_kg_arg, bwd_vg_arg, bwd_l_arg, bwd_d_arg,
                                ATTN_N, (QO_HEADS / KV_HEADS)};

    // Set dynamic shared memory size and other kernel attributes.
    int carveout_memsize = 194000; 
    cudaFuncSetAttribute(bwd_attend_ker<ATTN_D, causal>, cudaFuncAttributeMaxDynamicSharedMemorySize, carveout_memsize);
    cudaFuncSetAttribute(bwd_attend_ker<ATTN_D, causal>, cudaFuncAttributePreferredSharedMemoryCarveout, 85);

    // Define grid and block dimensions for backward kernel.
    dim3 grid_bwd(ATTN_N/(4 * BWD_CONSUMER_WARPGROUPS * kittens::TILE_ROW_DIM<bf16>), QO_HEADS, ATTN_B);
    dim3 block(32 * BWD_NUM_WORKERS);

    // Warmup and benchmark backward kernel.
    for (int i = 0; i < ITER; i++) {
        bwd_attend_ker<ATTN_D, causal><<<grid_bwd, block, carveout_memsize>>>(bwd_global);
    }
    cudaDeviceSynchronize();

    auto start = std::chrono::high_resolution_clock::now();
    for (int i = 0; i < ITER; i++) {
        bwd_attend_ker<ATTN_D, causal><<<grid_bwd, block, carveout_memsize>>>(bwd_global);
    }
    cudaDeviceSynchronize();
    auto finish = std::chrono::high_resolution_clock::now();

    // Copy gradients back from device.
    float* qg = new float[TOTAL_ELEMENTS_QO];
    float* kg = new float[TOTAL_ELEMENTS_KV];
    float* vg = new float[TOTAL_ELEMENTS_KV];
    cudaMemcpy(qg, d_qg, TOTAL_ELEMENTS_QO * sizeof(float), cudaMemcpyDeviceToHost);
    cudaMemcpy(kg, d_kg, TOTAL_ELEMENTS_KV * sizeof(float), cudaMemcpyDeviceToHost);
    cudaMemcpy(vg, d_vg, TOTAL_ELEMENTS_KV * sizeof(float), cudaMemcpyDeviceToHost);

    // Compare computed gradients against reference.
    double total_diff_qg = 0.0, max_diff_qg = 0.0;
    for (int i = 0; i < TOTAL_ELEMENTS_QO; i++) {
        double diff = std::fabs(qg[i] - qg_ref[i]);
        total_diff_qg += diff;
        if (diff > max_diff_qg) max_diff_qg = diff;
    }
    double total_diff_kg = 0.0, max_diff_kg = 0.0;
    for (int i = 0; i < TOTAL_ELEMENTS_KV; i++) {
        double diff = std::fabs(kg[i] - kg_ref[i]);
        total_diff_kg += diff;
        if (diff > max_diff_kg) max_diff_kg = diff;
    }
    double total_diff_vg = 0.0, max_diff_vg = 0.0;
    for (int i = 0; i < TOTAL_ELEMENTS_KV; i++) {
        double diff = std::fabs(vg[i] - vg_ref[i]);
        total_diff_vg += diff;
        if (diff > max_diff_vg) max_diff_vg = diff;
    }
    std::cout << "Backward Test Results:" << std::endl;
    std::cout << "  qg: Average diff = " << total_diff_qg / TOTAL_ELEMENTS_QO << ", Max diff = " << max_diff_qg << std::endl;
    std::cout << "  kg: Average diff = " << total_diff_kg / TOTAL_ELEMENTS_KV << ", Max diff = " << max_diff_kg << std::endl;
    std::cout << "  vg: Average diff = " << total_diff_vg / TOTAL_ELEMENTS_KV << ", Max diff = " << max_diff_vg << std::endl;

    auto time_us = std::chrono::duration_cast<std::chrono::microseconds>(finish - start).count() / ITER;
    std::cout << "  Average backward kernel execution time: " << time_us << " us" << std::endl;

    // Cleanup backward test allocations.
    delete[] qg_ref; delete[] kg_ref; delete[] vg_ref;
    delete[] qg; delete[] kg; delete[] vg;
    cudaFree(d_qg); cudaFree(d_kg); cudaFree(d_vg);
//}

//---------------------------------------------------------------------
// Main: choose forward or backward test (or both)
//---------------------------------------------------------------------
int main(int argc, char** argv) {
    //if (argc < 2) {
    //    std::cerr << "Usage: " << argv[0] << " [forward|backward|both] test_file.txt" << std::endl;
    //    return -1;
    //}
    //std::string mode = argv[1];
    //const char* testFile = argv[2];

    //std::cout << "Mode: " << mode << ", File: " << testFile << std::endl;

    //if (mode == "forward") {
    //    runForwardTest(testFile);
    //} else if (mode == "backward") {
    //    runBackwardTest(testFile);
    //} else if (mode == "both") {
    //    runForwardTest(testFile);
    //    runBackwardTest(testFile);
    //} else {
    //    std::cerr << "Invalid mode. Use 'forward', 'backward', or 'both'." << std::endl;
    //    return -1;
    //}
    return 0;
}
