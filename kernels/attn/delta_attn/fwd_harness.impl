#include <iostream>
#include <string>
#include <fstream>
// #include <cmath>
// #include <cassert>
// #include <chrono>
// #include <cuda_runtime.h>

// We'll assume you define bf16 somewhere, e.g. in "kittens.cuh"
// #include "kittens.cuh" 

// If your forward kernel and global definitions live in "delta_attn.cu" or "delta_attn.cuh", 
// you might need something like:
// #include "delta_attn.cuh"

// Adjust these as needed:
constexpr int ATTN_B     = 16;  // B
constexpr int QO_HEADS   = 16;  // Query/Output heads
constexpr int KV_HEADS   = 16;  // Key/Value heads
constexpr int ATTN_N     = 768; // Sequence length
// constexpr int ATTN_D     = 64;  // Head dimension
constexpr bool causal    = true;

static_assert(QO_HEADS == KV_HEADS, "For minimal code, set QO_HEADS == KV_HEADS.");

// If your kernel uses these:
#define NUM_WORKERS 8
#define BLOCK_SIZE  (32*NUM_WORKERS)
constexpr int ITER = 10;

// Simple CUDA error checking
#define CudaCheckError() __cudaCheckError(__FILE__, __LINE__)
inline void __cudaCheckError(const char* file, int line) {
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        std::cerr << "CUDA error at " << file << ":" << line << " -> "
                  << cudaGetErrorString(err) << std::endl;
        exit(-1);
    }
    err = cudaDeviceSynchronize();
    if (err != cudaSuccess) {
        std::cerr << "CUDA sync error at " << file << ":" << line << " -> "
                  << cudaGetErrorString(err) << std::endl;
        exit(-1);
    }
}

// Example function to compute a “fake” flop count
// long long flops_fwd(int B, int N, int D, int nheads) {
//     // Arbitrary estimate: Q @ K^T + (Q@K^T) @ V => ~2*N*N*D for each head
//     // Times B & nheads:
//     long long f = 2LL * B * nheads * N * N * D;
//     // E.g. if you want to consider "causal" => maybe f/=2;
//     return f;
// }
long long flops_ref(int B, int N, int H) {
    // Q@K.T and (Q@K.T)@V for a parallel view
    long long flops = 2LL * B * H * N * N * ATTN_D 
                      + 2LL * B * H * N * N * ATTN_D;
    return flops;
}

long long flops_real(int B, int N, int H) {
    // Flops for a chunk-wise strategy.
    int chunk_size = ACTIVE_TILES * ROWS;
    int n_chunks = N / chunk_size;
    long long flops = n_chunks * (2LL * B * H * chunk_size * chunk_size * ATTN_D 
                                  + 2LL * B * H * chunk_size * chunk_size * ATTN_D);
    return flops;
}


// Convert microseconds to TFLOPS
double efficiency(long long flop, double time_us) {
    // flop -> TFlop ; time_us -> ms
    double tflop = flop / 1e12;      // from flop to TFlop
    double ms    = time_us / 1e3;    // from microseconds to milliseconds
    return tflop / (ms / 1e3);       // TFlop / seconds
    // Equivalently: ( tflop / (ms*1e-3) )
}

int main(int argc, char** argv) {
    if (argc < 2) {
        std::cerr << "Usage: ./fwd_harness_minimal <test_file.txt>" << std::endl;
        return -1;
    }
    const char* filename = argv[1];

    // Derived sizes
    const int TOTAL_ELEMENTS_QO = ATTN_B * QO_HEADS * ATTN_N * ATTN_D;
    const int TOTAL_ELEMENTS_KV = ATTN_B * KV_HEADS * ATTN_N * ATTN_D;
    const int UNIQUE_ELEMENTS_QO = ATTN_N * ATTN_D; // single-head portion
    const int UNIQUE_ELEMENTS_KV = ATTN_N * ATTN_D;

    // Allocate host buffers for single-head (the “unique” data)
    float* h_q     = new float[UNIQUE_ELEMENTS_QO];
    float* h_k     = new float[UNIQUE_ELEMENTS_KV];
    float* h_v     = new float[UNIQUE_ELEMENTS_KV];
    float* h_o_ref = new float[UNIQUE_ELEMENTS_QO];

    // Large buffers for the final (replicated) device input
    bf16*  h_q_bf  = new bf16[TOTAL_ELEMENTS_QO];
    bf16*  h_k_bf  = new bf16[TOTAL_ELEMENTS_KV];
    bf16*  h_v_bf  = new bf16[TOTAL_ELEMENTS_KV];
    bf16*  h_o_bf  = new bf16[TOTAL_ELEMENTS_QO];  // for final forward kernel output

    // Host memory for final output in float
    float* h_o     = new float[TOTAL_ELEMENTS_QO];

    // Read from file
    std::ifstream infile(filename);
    if (!infile.is_open()) {
        std::cerr << "Could not open " << filename << std::endl;
        return -1;
    }
    // For minimal code, read Q, then K, then V, then O_ref:
    for(int i=0; i<UNIQUE_ELEMENTS_QO; i++) infile >> h_q[i];
    for(int i=0; i<UNIQUE_ELEMENTS_KV; i++) infile >> h_k[i];
    for(int i=0; i<UNIQUE_ELEMENTS_KV; i++) infile >> h_v[i];
    for(int i=0; i<UNIQUE_ELEMENTS_QO; i++) infile >> h_o_ref[i];
    std::cout << "Finished loading file from " << argv[1] << "!" << std::endl;
    // infile.close();

    // Replicate across batch & heads
    for(int i=0; i < TOTAL_ELEMENTS_QO; i++){
        // Q portion
        int idx = i % UNIQUE_ELEMENTS_QO;
        h_q_bf[i] = __float2bfloat16(h_q[idx]);
    }
    for(int i=0; i < TOTAL_ELEMENTS_KV; i++){
        // K portion
        int idx = i % UNIQUE_ELEMENTS_KV;
        h_k_bf[i] = __float2bfloat16(h_k[idx]);
        // V portion
        h_v_bf[i] = __float2bfloat16(h_v[idx]);
    }

    // Device memory
    bf16 *d_q, *d_k, *d_v, *d_o;
    cudaMalloc(&d_q, TOTAL_ELEMENTS_QO * sizeof(bf16));
    cudaMalloc(&d_k, TOTAL_ELEMENTS_KV * sizeof(bf16));
    cudaMalloc(&d_v, TOTAL_ELEMENTS_KV * sizeof(bf16));
    cudaMalloc(&d_o, TOTAL_ELEMENTS_QO * sizeof(bf16));

    // Copy
    cudaMemcpy(d_q, h_q_bf, TOTAL_ELEMENTS_QO * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_k, h_k_bf, TOTAL_ELEMENTS_KV * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_v, h_v_bf, TOTAL_ELEMENTS_KV * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemset(d_o, 0, TOTAL_ELEMENTS_QO * sizeof(bf16));

    // Build minimal forward “globals” structure.
    // We assume you have something like:
    // fwd_globals fwd_init(bf16*,bf16*,bf16*,bf16*,unsigned long B, unsigned long H, unsigned long N);
    // And a kernel named: template<int HEAD_DIM, bool CAUSAL> __global__ void fwd_attend_ker(...).
    // Adjust as necessary.

    fwd_globals g = fwd_init(
        d_q, d_k, d_v, d_o,
        ATTN_B, QO_HEADS, ATTN_N
    );

    cudaDeviceSynchronize();
    CudaCheckError();
    
    // Launch
    unsigned long mem_size = kittens::MAX_SHARED_MEMORY; // or kittens::MAX_SHARED_MEMORY
    std::cout << "Setting max block shared memory to " << mem_size << std::endl;

    // If your kernel is called “fwd_attend_ker<ATTN_D, causal>”:
    cudaFuncSetAttribute(
        delta_attention_fwd<ATTN_D, causal>,
        cudaFuncAttributeMaxDynamicSharedMemorySize,
        mem_size
    );
    CudaCheckError();
    cudaDeviceSynchronize();

    // Suppose your kernel’s grid is (QO_HEADS, ATTN_B) and block = BLOCK_SIZE
    // or do something else:
    dim3 grid(QO_HEADS, ATTN_B);
    dim3 block(BLOCK_SIZE);

    // Warmup
    std::cout << "Warmup!\n";
    for(int i=0; i<ITER; i++){
        delta_attention_fwd<<<grid, NUM_THREADS, mem_size>>>(g);
    }
    cudaDeviceSynchronize();
    CudaCheckError();

    std::cout << "Starting main forward kernel...\n";
    auto start = std::chrono::high_resolution_clock::now();
    for(int i=0; i<ITER; i++){
        delta_attention_fwd<<<grid, NUM_THREADS, mem_size>>>(g);
    }

    cudaDeviceSynchronize();
    auto finish = std::chrono::high_resolution_clock::now();
    CudaCheckError();

    double time_us = std::chrono::duration_cast<std::chrono::microseconds>(finish - start).count() / (double)ITER;
    std::cout << "Done. Average time: " << time_us << " us\n";


    /*************************** Correctness Checks **************************** */
    
    // cudaMemcpy(h_o_bf, d_o, TOTAL_ELEMENTS_QO * sizeof(bf16), cudaMemcpyDeviceToHost);
    // for(int i=0; i < TOTAL_ELEMENTS_QO; i++){
    //     h_o[i] = __bfloat162float(h_o_bf[i]);
    // }

    // // Compare unique portion
    // double total_diff = 0.0;
    // double max_diff   = 0.0;
    // for(int i=0; i<UNIQUE_ELEMENTS_QO; i++){
    //     double diff = h_o[i] - h_o_ref[i];
    //     total_diff += std::fabs(diff);
    //     if(std::fabs(diff) > max_diff) max_diff = std::fabs(diff);
    // }
    // std::cout << "Average diff: " << (total_diff / UNIQUE_ELEMENTS_QO) << "\n"
    //           << "Max diff: " << max_diff << "\n";

    // // Efficiency
    // long long flop = flops_fwd(ATTN_B, ATTN_N, ATTN_D, QO_HEADS);
    // double tflops  = efficiency(flop, time_us);
    // std::cout << "Approx. Efficiency: " << tflops << " TFLOPS\n";

    // Copy device output to host BF16 buffer and convert to float.
    cudaMemcpy(h_o_bf, d_o, TOTAL_ELEMENTS_QO * sizeof(bf16), cudaMemcpyDeviceToHost);
    for (int i = 0; i < TOTAL_ELEMENTS_QO; i++) {
        h_o[i] = __bfloat162float(h_o_bf[i]);
    }

    // Detailed diagnostic: write outputs to files and compute error metrics.
    bool good = true;
    std::ofstream o_ref_file("printouts/o_ref.txt");
    std::ofstream o_file("printouts/o.txt");
    std::ofstream diff_file("printouts/diff.txt");

    double total_diff = 0.0;
    double max_diff = 0.0;

    std::cout << "Total elements (full tensor): " << TOTAL_ELEMENTS_QO << std::endl;
    std::cout << "Unique elements (per-head): " << UNIQUE_ELEMENTS_QO << std::endl;

    // Print first 10 outputs for sanity checking.
    for (int i = 0; i < 10; i++) {
        std::cout << "o[" << i << "] = " << h_o[i] 
                << "    o_ref[" << i << "] = " << h_o_ref[i] << std::endl;
    }

    // Compare only the unique portion (e.g. one head’s data).
    for (int i = 0; i < UNIQUE_ELEMENTS_QO; i++) {
        float diff = h_o[i] - h_o_ref[i];
        // Write the reference, computed, and difference values to files.
        o_ref_file << h_o_ref[i] << ' ';
        o_file << h_o[i] << ' ';
        diff_file << diff << ' ';

        if (fabs(diff) > 0.05 || isnan(diff)) {
            good = false;
        }
        total_diff += fabs(diff);
        if (fabs(diff) > max_diff) {
            max_diff = fabs(diff);
        }
    }
    o_ref_file.close();
    o_file.close();
    diff_file.close();

    double avg_diff = total_diff / UNIQUE_ELEMENTS_QO;
    std::cout << "Max difference: " << max_diff << std::endl;
    std::cout << "Average difference: " << avg_diff << std::endl;
    if (good)
        std::cout << "Output is correct :)" << std::endl;
    else
        std::cout << "Output is incorrect :(" << std::endl;

    // Efficiency calculations.
    double avg_exec_time = std::chrono::duration_cast<std::chrono::microseconds>(finish - start).count() / (double)ITER;
    std::cout << "Average kernel execution time: " << avg_exec_time << " us" << std::endl;

    // long long flop_ref = flops_fwd(ATTN_B, ATTN_N, ATTN_D, QO_HEADS);
    // long long flop_real = flops_real(ATTN_B, ATTN_N, ATTN_D, QO_HEADS);
    // long long f_ref = flops_ref(ATTN_B, ATTN_N, ATTN_H);
    // long long f_real = flops_real(ATTN_B, ATTN_N, ATTN_H); 

    // double e_ref = efficiency(flop_ref, avg_exec_time);
    // double e_real = efficiency(flop_real, avg_exec_time);
    // std::cout << "Efficiency (ref): " << e_ref << " TFLOPS" << std::endl;
    // std::cout << "Efficiency (real): " << e_real << " TFLOPS" << std::endl;

    // Cleanup
    cudaFree(d_q);
    cudaFree(d_k);
    cudaFree(d_v);
    cudaFree(d_o);

    delete[] h_q;
    delete[] h_k;
    delete[] h_v;
    delete[] h_o_ref;
    delete[] h_q_bf;
    delete[] h_k_bf;
    delete[] h_v_bf;
    delete[] h_o_bf;
    delete[] h_o;

    return 0;
}
